# CS336 Assignment 2 â€“ Systems & Parallelism for Transformer Training

æœ¬é¡¹ç›®æ˜¯æ–¯å¦ç¦å¤§å­¦ **CS336ã€ŠLanguage Modeling from Scratchã€‹Spring 2025** è¯¾ç¨‹ç¬¬äºŒæ¬¡ä½œä¸šï¼ˆAssignment 2: Systemsï¼‰çš„å®Œæ•´å®ç°ã€‚
é¡¹ç›®åœ¨ Assignment 1 ä»é›¶å®ç° Transformer è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé‡ç‚¹å…³æ³¨ **å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ç³»ç»Ÿæ€§èƒ½é—®é¢˜**ï¼Œå›´ç»• **GPU åˆ©ç”¨ç‡ã€æ˜¾å­˜å ç”¨ã€å¤šå¡é€šä¿¡ä¸å¹¶è¡Œè®­ç»ƒ** ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œé€æ­¥å®ç°å¹¶éªŒè¯äº†ä¸€ç³»åˆ—ç³»ç»Ÿçº§ä¼˜åŒ–æ–¹æ³•ã€‚

---

## é¡¹ç›®äº®ç‚¹ï¼ˆHighlightsï¼‰

* ğŸ” **Profiling-driven optimization**ï¼šä»æ€§èƒ½åˆ†æå…¥æ‰‹ï¼Œè€Œéç›²ç›®ä¼˜åŒ–
* âš¡ **FlashAttention-2ï¼ˆTriton å®ç°ï¼‰**ï¼šæ”¯æŒè¶…é•¿åºåˆ—çš„é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
* ğŸ”— **ä»é›¶å®ç° DDP ä¸ Bucket DDP**ï¼šæ‰‹å†™æ¢¯åº¦åŒæ­¥ä¸é€šä¿¡é€»è¾‘ï¼Œæ·±å…¥ç†è§£åˆ†å¸ƒå¼è®­ç»ƒ
* ğŸ”„ **é€šä¿¡ä¸è®¡ç®—é‡å ï¼ˆOverlapï¼‰**ï¼šæ˜¾è‘—é™ä½åˆ†å¸ƒå¼è®­ç»ƒå¼€é”€
---




# ç»“æœ
## 1ã€ç«¯åˆ°ç«¯åŸºå‡†æµ‹è¯•å®éªŒï¼š
```python
MODEL_CONFIGS = {
    "small": dict(d_model=768, d_ff=3072, num_layers=12, num_heads=12),
    "medium": dict(d_model=1024, d_ff=4096, num_layers=24, num_heads=16),
    "large": dict(d_model=1280, d_ff=5120, num_layers=36, num_heads=20),
    "xl": dict(d_model=1600, d_ff=6400, num_layers=48, num_heads=25),
    "2.7b": dict(d_model=2560, d_ff=10240, num_layers=32, num_heads=32),
}
```
### æ¨¡å‹æ¨ç†/è®­ç»ƒæ€§èƒ½æµ‹è¯•ç»“æœ
| æ¨¡å‹å°ºå¯¸ | æ“ä½œç±»å‹          | æ˜¯å¦ä½¿ç”¨Compile | å‡å€¼(ms) | æ ‡å‡†å·®(ms) |
|----------|-------------------|-----------------|----------|------------|
| small    | forward+backward  | å¦              | 38.44    | 0.14       |
| medium   | forward+backward  | å¦              | 110.16   | 0.08       |
| large    | forward+backward  | å¦              | 221.07   | 0.19       |
| xl       | forward+backward  | å¦              | 447.25   | 0.39       |
| 2.7b     | forward+backward  | å¦              | 680.90   | 0.63       |
| small    | forward-only      | å¦              | 18.34    | 1.30       |
| medium   | forward-only      | å¦              | 36.30    | 0.13       |
| large    | forward-only      | å¦              | 70.29    | 0.21       |
| xl       | forward-only      | å¦              | 144.03   | 0.62       |
| 2.7b     | forward-only      | å¦              | 222.46   | 0.25       |
| small    | forward+backward  | æ˜¯              | 32.20    | 2.19       |
| medium   | forward+backward  | æ˜¯              | 96.29    | 0.48       |
| large    | forward+backward  | æ˜¯              | 201.70   | 0.68       |
| xl       | forward+backward  | æ˜¯              | 414.44   | 0.71       |
| 2.7b     | forward+backward  | æ˜¯              | 652.06   | 1.34       |
| small    | forward-only      | æ˜¯              | 9.33     | 4.23       |
| medium   | forward-only      | æ˜¯              | 32.53    | 2.88       |
| large    | forward-only      | æ˜¯              | 62.96    | 1.99       |
| xl       | forward-only      | æ˜¯              | 130.94   | 0.25       |
| 2.7b     | forward-only      | æ˜¯              | 212.69   | 0.34       |

### æ€»ç»“
1. `torch.compile` å¯¹æ‰€æœ‰æ¨¡å‹å°ºå¯¸çš„æ“ä½œå‡æœ‰åŠ é€Ÿæ•ˆæœï¼Œå…¶ä¸­smallæ¨¡å‹çš„forward-onlyåŠ é€Ÿæœ€æ˜¾è‘—ï¼ˆä»18.34msé™è‡³9.33msï¼‰ï¼›
2. æ¨¡å‹å°ºå¯¸è¶Šå¤§ï¼Œforward+backwardçš„è€—æ—¶å¢é•¿è¶Šæ˜æ˜¾ï¼Œä¸”æ ‡å‡†å·®æ•´ä½“éšæ¨¡å‹å°ºå¯¸å¢å¤§ç•¥æœ‰ä¸Šå‡ï¼›
3. ä½¿ç”¨compileåï¼Œéƒ¨åˆ†åœºæ™¯ï¼ˆå¦‚smallæ¨¡å‹ï¼‰çš„æ ‡å‡†å·®æœ‰æ‰€å¢å¤§ï¼Œå¯èƒ½æ˜¯ç¼–è¯‘ä¼˜åŒ–çš„åŠ¨æ€ç‰¹æ€§å¯¼è‡´çš„å°å¹…æ³¢åŠ¨ã€‚

## 2ã€nsyså®éªŒ
ç”±äºä½¿ç”¨çš„æœåŠ¡å™¨å¹¶æ²¡æœ‰å®‰è£…ï¼Œä¹Ÿæ²¡æœ‰rootæƒé™ï¼Œåªèƒ½é—æ†¾è·³è¿‡è¯¥éƒ¨åˆ†ã€‚


## 3ã€ç²¾åº¦å®éªŒ
è¿è¡Œä»¥ä¸‹ä»£ç ï¼š
```python
import torch

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000): 
    s += torch.tensor(0.01,dtype=torch.float32)
print(s)

s = torch.tensor(0,dtype=torch.float16)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s) 

s = torch.tensor(0,dtype=torch.float32) 
for i in range(1000): 
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32) 
for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16) 
    s += x.type(torch.float32)
print(s) 
```
å¾—åˆ°è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
```python
tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
```
æ¯«æ— ç–‘é—®ï¼Œå®Œå…¨ä½¿ç”¨f32çš„è®¡ç®—ç²¾åº¦æ˜¯æœ€é«˜çš„ã€‚è€Œå°†f16è½¬æ¢æˆf32çš„ç²¾åº¦æŸå¤±ç›¸æ¯”å®Œå…¨ä½¿ç”¨f16çš„ç²¾åº¦ä¹Ÿè¿˜æ˜¯æ›´é«˜ä¸€äº›ã€‚

## 4ã€æ··åˆç²¾åº¦å®éªŒ
è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä½¿ç”¨autocastæŒ‡å®šè¿ç®—ç²¾åº¦ä¸ºbf16ã€‚è§‚å¯Ÿå“ªäº›éƒ¨åˆ†è¢«è½¬æ¢æˆbf16äº†ï¼Œå“ªäº›ä»ç„¶æ˜¯f32ï¼š
```python
class ToyModel(nn.Module):
    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        self.fc1 = nn.Linear(in_features, 10, bias=False)
        self.ln = nn.LayerNorm(10)
        self.fc2 = nn.Linear(10, out_features, bias=False)
        self.relu = nn.ReLU()
    def forward(self, x):
        print(f'fc1 weight : {self.fc1.weight.dtype}')
        print(f'ln weight : {self.ln.weight.dtype}')
        print(f'fc2 weight : {self.fc2.weight.dtype}')
        x = self.relu(self.fc1(x))
        print('fc1 å', x.dtype)
        x = self.ln(x)
        print('ln å', x.dtype)
        x = self.fc2(x)
        print('fc2 å', x.dtype)
        return x
criterion = nn.MSELoss()
model = ToyModel(4, 4).to('cuda')
x = torch.rand(4, 4, dtype=torch.float32).to('cuda')
with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
    y = model(x)
    target = torch.zeros_like(y)
    loss = criterion(y, target)
    print('loss dtype:', loss.dtype)
    loss.backward()
    print('fc1.grad dtype:', model.fc1.weight.grad.dtype)
    print('ln.grad dtype:', model.ln.weight.grad.dtype)
    print('fc2.grad dtype:', model.fc2.weight.grad.dtype)
```

ç»“æœå¦‚ä¸‹ï¼š
```python
fc1 weight : torch.float32
ln weight : torch.float32
fc2 weight : torch.float32
fc1 å torch.bfloat16
ln å torch.float32
fc2 å torch.bfloat16
loss dtype: torch.float32
fc1.grad dtype: torch.float32
ln.grad dtype: torch.float32
fc2.grad dtype: torch.float32
```
è§‚å¯Ÿç»“è®ºå¯ä»¥å‘ç°ï¼ŒAMPï¼ˆautocastï¼‰åªé™ä½â€œç®—å­å†…éƒ¨è®¡ç®—å’Œæ¿€æ´»â€çš„ç²¾åº¦ï¼Œä¸é™ä½â€œå‚æ•°ã€å½’ä¸€åŒ–ã€lossã€æ¢¯åº¦â€çš„ç²¾åº¦ã€‚
åœ¨æ‰§è¡Œçº¿æ€§å±‚çš„æ—¶å€™ï¼ˆçŸ©é˜µä¹˜æ³•è¿™ç§è®¡ç®—å¯†é›†å‹ç®—å­ï¼‰æ—¶ï¼Œautocastè¿›è¡Œç²¾åº¦è½¬æ¢ï¼ŒåŒ…æ‹¬æ¿€æ´»åçš„ç±»å‹ä¹Ÿæ˜¯è½¬æ¢åçš„ç²¾åº¦ï¼š
```python
x(fp32) Ã— W(fp32)
  â†“ autocast
x(bf16) Ã— W(bf16) â†’ output(bf16)
```
è€Œåœ¨layerNormè¿™ç§æ•°å€¼æ•æ„Ÿç®—å­ä¼šå¼ºåˆ¶å›é€€åˆ° float32 è®¡ç®—ï¼Œå¹¶è¾“å‡º float32 æ¿€æ´»ã€‚

## 5ã€å†…å­˜å®éªŒ
ä»…forwardï¼š
![alt text](image.png)
å¯ä»¥çœ‹åˆ°æ¯æ¬¡forwardæ—¶æŠµè¾¾ä¸€æ¬¡æ³¢å³°ï¼Œæœ‰è¿‡å¤šå°‘æ¬¡forwardå°±æœ‰å¤šå°‘æ¬¡æ³¢å³°

forward + backwardï¼š
![alt text](image-1.png)
è¿™æ¡è®°å½•å¯¹åº”çš„æ˜¯ï¼šåœ¨ backward é˜¶æ®µï¼Œä¸ºå‚æ•°åˆ›å»º / ç´¯ç§¯æ¢¯åº¦ï¼ˆ.gradï¼‰è€Œè¿›è¡Œçš„ä¸€æ¬¡ CUDA å†…å­˜åˆ†é…ã€‚
å…·ä½“è€Œè¨€ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œautograd å¼•æ“æ‰§è¡Œåˆ°æŸä¸ªå‚æ•°çš„ AccumulateGrad èŠ‚ç‚¹æ—¶ï¼Œå‘ç°è¯¥å‚æ•°çš„ .grad è¿˜ä¸å­˜åœ¨ï¼Œäºæ˜¯è°ƒ empty_strided_cuda åœ¨ GPU ä¸Šåˆ†é…äº†ä¸€å—çº¦ 29.3MiB çš„è¿ç»­æ˜¾å­˜ï¼Œç”¨äºå­˜å‚¨è¯¥å‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶å°†åç»­åå‘ä¼ æ’­è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ç´¯åŠ åˆ°è¯¥ buffer ä¸­ã€‚

ä» CUDA å†…å­˜æ—¶é—´çº¿å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å­˜ä½¿ç”¨å‘ˆç°ç¨³å®šçš„å‘¨æœŸæ€§é”¯é½¿ç»“æ„ã€‚æ¯ä¸ªè®­ç»ƒè¿­ä»£ä¸­ï¼Œå‰å‘ä¼ æ’­é˜¶æ®µé€æ­¥åˆ†é…ä¸­é—´æ¿€æ´»ï¼Œåå‘ä¼ æ’­é˜¶æ®µè¾¾åˆ°æ˜¾å­˜å³°å€¼ï¼Œéšååœ¨æ¢¯åº¦è®¡ç®—å®Œæˆåæ˜¾å­˜å›è½ã€‚æ•´ä½“æ˜¾å­˜åŸºçº¿ä¿æŒç¨³å®šï¼Œæœªå‡ºç°æŒç»­å¢é•¿ï¼Œè¡¨æ˜ä¸å­˜åœ¨æ˜¾è‘—çš„å†…å­˜æ³„æ¼é—®é¢˜ã€‚


## 6ã€åŸå§‹Attention | pytorchå®ç°FlashAttention | Tritonå®ç°FlashAttention æ€§èƒ½æ¯”è¾ƒ
| T | D | dtype | PT fwd (ms) | PT bwd (ms) | PT fwd+bwd (ms) | TR fwd (ms) | TR bwd (ms) | TR fwd+bwd (ms) | Naive fwd (ms) | Naive bwd (ms) | Naive fwd+bwd (ms) |
|---|---|-------|----|----|----|----|----|----|----|----|----|
| 128 | 16 | bfloat16 | 8.445 | 0.103 | 9.405 | 0.011 | 0.181 | 0.553 | 0.034 | 0.164 | 0.573 |
| 128 | 16 | float32 | 8.677 | 0.265 | 9.294 | 0.010 | 0.362 | 0.591 | 0.042 | 0.148 | 0.572 |
| 128 | 32 | bfloat16 | 8.586 | 0.272 | 9.415 | 0.012 | 0.383 | 0.598 | 0.040 | 0.149 | 0.577 |
| 128 | 32 | float32 | 8.511 | 0.293 | 9.292 | 0.012 | 0.357 | 0.585 | 0.042 | 0.149 | 0.599 |
| 256 | 16 | bfloat16 | 32.762 | 0.267 | 33.762 | 0.015 | 0.382 | 0.602 | 0.042 | 0.159 | 0.592 |
| 256 | 16 | float32 | 32.770 | 0.272 | 33.612 | 0.014 | 0.370 | 0.631 | 0.044 | 0.152 | 0.569 |
| 256 | 32 | bfloat16 | 34.085 | 0.278 | 34.004 | 0.017 | 0.377 | 0.607 | 0.041 | 0.160 | 0.601 |
| 256 | 32 | float32 | 33.080 | 0.278 | 33.762 | 0.018 | 0.355 | 0.594 | 0.046 | 0.161 | 0.579 |

å¯ä»¥çœ‹è§ï¼Œä½¿ç”¨Tritonå®ç°çš„FlashAttention Forwardæ€§èƒ½æ˜¾è‘—ä¼˜äºpytorchå®ç°ä¸åŸAttentionå®ç°
å¯ä»¥é¢„è§ï¼Œä½¿ç”¨Tritonå®ç°FlashAttention çš„ Backwardï¼Œæ€§èƒ½ä¹Ÿèƒ½å¤§å¹…ä¼˜åŒ–ï¼Œæœ¬é¡¹ç›®ä¸è¿›è¡Œå®ç°ï¼Œæœ‰æœºä¼šæ¥å®Œæˆã€‚

## 7ã€Final: å®Œæˆäº†æœ¬é¡¹ç›®ä¸­çš„å¹¶è¡Œéƒ¨åˆ†ï¼Œå®ç°äº† DDP å’Œ Bucket DDP
è¿™äº›å†…å®¹æœ‰å¯¹åº”çš„pytestæµ‹è¯•ç”¨ä¾‹ï¼Œå®Œå…¨é€šè¿‡ã€‚
ï¼ˆä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡æœªå†™ï¼Œæœ‰æœºä¼šæ¥å®Œæˆï¼‰